{"cells":[{"cell_type":"markdown","metadata":{"id":"S67uwxlYL__i"},"source":["# Finetuning of a Faster RCNN for object detection\n","\n","In this task, a [Faster RCNN](https://arxiv.org/abs/1506.01497) for object detection is re-trained on a new dataset.\n","\n","An intensive use of [PyTorch](https://pytorch.org/)'s functionalities has been used. Among other things, I used a pre-trained network and fine-tune a network with new data.\n","\n","**Note:** The Notbook was developed on a Linux environment and using Google-Colab. It can lead to unpredictability under Windows. Accordingly, running in Colab is recommended.\n"]},{"cell_type":"markdown","metadata":{"id":"49hJYk6yfhOm"},"source":["When using Google Colab set the flag `USE_COLAB` to `True`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwVdzroqqOYz"},"outputs":[],"source":["USE_COLAB = True\n","TARGET_SIZE_IM = (256,256)\n","BATCH_SIZE = 32"]},{"cell_type":"markdown","source":["Set the current working directory here if using google colab"],"metadata":{"id":"RJlScJvmUALl"}},{"cell_type":"code","source":["zip_file_path = '/content/drive/MyDrive/Capstone Project/Data/FoodLogoDet-1500.zip'\n","curren_dir = '/content/drive/MyDrive/Colab Notebooks/'"],"metadata":{"id":"Ybit1kj9T1PZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wk0I6WYpqOY0"},"source":["Import all the libraries used.\n","Please note to set the path to the exercise folder correctly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-KgAi3Fif9S","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"50217eb6-1e8a-4fcc-8ff9-664ac468b29c","executionInfo":{"status":"error","timestamp":1730681047956,"user_tz":300,"elapsed":174,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}}},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6d9c9e244643>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."]}],"source":["import sys\n","if USE_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","    sys.path.append(zip_file_path)\n","else:\n","    sys.path.append('')\n","\n","import numpy as np\n","import cv2\n","import os\n","from datetime import datetime\n","import zipfile\n","from PIL import Image\n","from io import BytesIO\n","import xml.etree.ElementTree as ET\n","import re\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torch.utils.data import Dataset, DataLoader, Subset\n","import torchvision.transforms.functional as F\n","\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","import pickle\n","\n","import pandas as pd\n","import glob as glob\n","import random\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"mfY-7hMUoI55"},"source":["## 1. Detection with a pre-trained model"]},{"cell_type":"markdown","metadata":{"id":"RVEe1xomftEK"},"source":["Selecting the device (CPU or GPU) on which to run the infernce and training.\n","This notebook is set to use the GPU under Google Colab.\n","\n","Download the pre-trained **Faster R-CNN** with ResNet50 Backbone, on the COCO dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFERCLWbjafV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5583e97-94b3-4d66-e6ed-90374cd83da2","executionInfo":{"status":"ok","timestamp":1730165315682,"user_tz":240,"elapsed":1808,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["We use the following device:  cuda\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:00<00:00, 168MB/s]\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"We use the following device: \", device)\n","# load a model; pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1').to(device)"]},{"cell_type":"markdown","metadata":{"id":"pzgtASZDlh7l"},"source":["Function that draws the BBoxes, scores, and labels on the image.\n"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"5q7JEhaurymN","executionInfo":{"status":"ok","timestamp":1730731031710,"user_tz":300,"elapsed":168,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}}},"outputs":[],"source":["import matplotlib.patches as patches\n","def plot_image(img, boxes, scores, labels, dataset, save_path=None):\n","  '''\n","  Function that draws the BBoxes, scores, and labels on the image.\n","\n","  inputs:\n","    img: input-image as numpy.array (shape: [H, W, C])\n","    boxes: list of bounding boxes (Format [N, 4] => N times [xmin, ymin, xmax, ymax])\n","    scores: list of conf-scores (Format [N] => N times confidence-score between 0 and 1)\n","    labels: list of class-prediction (Format [N] => N times an number between 0 and _num_classes-1)\n","    dataset: list of all classes e.g. [\"background\", \"class1\", \"class2\", ..., \"classN\"] => Format [N_classes]\n","  '''\n","\n","  cmap = plt.get_cmap(\"tab20b\")\n","  class_labels = np.array(dataset)\n","  colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n","  height, width, _ = img.shape\n","  # Create figure and axes\n","  fig, ax = plt.subplots(1, figsize=(10, 6))\n","  # Display the image\n","  ax.imshow(img)\n","  for i, box in enumerate(boxes):\n","    class_pred = labels[i]\n","    conf = scores[i]\n","    width = box[2] - box[0]\n","    height = box[3] - box[1]\n","    rect = patches.Rectangle(\n","        (box[0], box[1]),\n","        width,\n","        height,\n","        linewidth=2,\n","        edgecolor=colors[int(class_pred)],\n","        facecolor=\"none\",\n","    )\n","    # Add the patch to the Axes\n","    ax.add_patch(rect)\n","    plt.text(\n","        box[0], box[1],\n","        s=class_labels[int(class_pred)] + \" \" + str(int(100*conf)) + \"%\",\n","        color=\"white\",\n","        verticalalignment=\"top\",\n","        bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n","    )\n","\n","  # Used to save inference phase results\n","  if save_path is not None:\n","    plt.savefig(save_path)\n","  plt.axis(\"off\")\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6YlA6epfpbQ7"},"source":["Image transformation and execution of the infernce."]},{"cell_type":"markdown","source":["##Collect the data frame with the bounding boxes"],"metadata":{"id":"udWHdFJ9Q8u_"}},{"cell_type":"code","source":["excel_file_path = '/content/drive/MyDrive/Capstone Project/Data/Food_boxes_224x224.xlsx'\n","\n","df_labels_resized = pd.read_excel(excel_file_path, dtype={'filename': str})\n","df_labels_resized.head()"],"metadata":{"id":"eAjXg0rgITHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mila = int(df_labels['filename'].max())\n","print(mila)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XzUHm16_zBK9","executionInfo":{"status":"ok","timestamp":1730165375192,"user_tz":240,"elapsed":5,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"dd595f98-3b2d-4177-92aa-f778e03fdc8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["99768\n"]}]},{"cell_type":"markdown","metadata":{"id":"T5DnF5vLBf7_"},"source":["## 2. Finetuning of a pre-trained model with new data"]},{"cell_type":"markdown","metadata":{"id":"mo_LhlZrb5o-"},"source":["### Dataset creation\n","\n","Use the default dataset of PyTorch to create the dataset class `vehicleDataset` with which the FasterRCNN can be trained. (https://pytorch.org/vision/main/models/faster_rcnn.html)\n","\n","**Note 1:** There are no labels for some images because they show an 'empty' scene &rarr; these images should be filtered out.\n","\n","**Note 2:** There are incorrect bounding boxes in the dataset (e.g. `xmax=xmin`)."]},{"cell_type":"markdown","source":["**Transformations (augmentations)**"],"metadata":{"id":"XGJgTvdeH3JA"}},{"cell_type":"markdown","source":["# Collect the resized images from the folder\n","\n"],"metadata":{"id":"clIdhZGnV-ID"}},{"cell_type":"markdown","metadata":{"id":"dKQkyGmiqOY6"},"source":["collect the path to all the images in one list"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","image_resized_folder = '/content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224'\n","NUM_FILES = 10000\n","\n","image_files = [None] * mila # tot_num_files\n","current_count = 0\n","\n","# Traverse the directory and subdirectories to collect image paths\n","for root, dirs, files in os.walk(image_resized_folder):\n","    print('root', root)\n","    for f in files:\n","        if f.endswith('.jpg'):\n","            # if current_count < NUM_FILES:\n","            image_files[current_count] = os.path.join(root, f)\n","            current_count += 1\n","            # else:\n","            #     break  # Stop if we've reached the limit\n","\n","image_files = image_files[:NUM_FILES]\n","\n","# Create a list with the index for the train set and the validation set\n","image_indices = np.arange(len(image_files))\n","train_indices, val_indices = train_test_split(image_indices, test_size=0.2, random_state=42)\n","\n","# Optionally print the number of images found and train/val splits\n","print(f\"Total images found: {len(image_files)}\")\n","print(f\"Training set size: {len(train_indices)}, Validation set size: {len(val_indices)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkpRZVY4DYIV","executionInfo":{"status":"ok","timestamp":1730165492892,"user_tz":240,"elapsed":110710,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"7845bfe9-5663-47a9-c3a7-8cd0b59def50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_0\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_1\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_2\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_3\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_4\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_5\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_6\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_7\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_8\n","root /content/drive/MyDrive/Capstone Project/Data/FoodDet_ResizedImages_224x224/folder_9\n","Total images found: 10000\n","Training set size: 8000, Validation set size: 2000\n"]}]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"zG_eqERKLtV0"}},{"cell_type":"code","source":["# Create a Dataloader for the images and the correspoding boxes and labels\n","\n","def custom_data_loader(folder_path, image_files, indices, labels_dict, batch_size, target_size):\n","    while True:  # Infinite loop for continuous data loading\n","        for start in range(0, len(indices), batch_size):\n","            end = min(start + batch_size, len(indices))\n","            batch_images = []\n","            batch_targets = []\n","\n","            for i in range(start, end):\n","                # Get the filename without the extension or path\n","                image_filename = os.path.splitext(os.path.basename(image_files[indices[i]]))[0]\n","\n","                # Retrieve boxes and labels from the dictionary\n","                image_annotations = labels_dict.get(image_filename)\n","                if image_annotations is None:\n","                    continue  # Skip if no annotations for the image\n","\n","                # Separate boxes and labels\n","                boxes = image_annotations[:, :4]  # xmin, ymin, xmax, ymax\n","                labels = image_annotations[:, 4]  # label column\n","\n","                # Create the target dictionary\n","                target = {\n","                    'boxes': torch.tensor(boxes, dtype=torch.float32),\n","                    'labels': torch.tensor(labels, dtype=torch.long)\n","                }\n","                batch_targets.append(target)\n","\n","                # Open and process the image\n","                image_path = os.path.join(folder_path, image_files[indices[i]])\n","                try:\n","                    with Image.open(image_path) as img:\n","                        img_data = np.array(img.convert('RGB'))\n","                        batch_images.append(img_data)\n","                except OSError as e:\n","                    print(f\"Warning: Could not process image {image_files[indices[i]]}. Error: {e}\")\n","\n","            # Convert images to tensor and normalize to [0,1]\n","            batch_images = torch.tensor(np.array(batch_images), dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n","\n","            yield batch_images, batch_targets\n","\n"],"metadata":{"id":"IGtcuQtwLxpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {\n","    filename: group[['xmin', 'ymin', 'xmax', 'ymax', 'label']].values\n","    for filename, group in df_labels.groupby('filename')\n","}"],"metadata":{"id":"QngO-TzJWopm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the Fast R-CNN model\n","\n","(run only the cell of the model you want to choose)"],"metadata":{"id":"0vb7DdY1mNJL"}},{"cell_type":"code","source":["import torch.optim as optim\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models import resnet18\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n"],"metadata":{"id":"LkfuZmi1muzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Backbone model: ResNet50 (nb params= 41,299,161)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print('device used:', device )\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n","num_classes = 2  # Logo + background\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.to(device)\n","\n","# Optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params}\")\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLvvoaTDLvSK","executionInfo":{"status":"ok","timestamp":1730165541833,"user_tz":240,"elapsed":1748,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"c99aee01-8b75-4a07-f895-d5845eeea0a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device used: cuda\n","Total number of parameters: 41299161\n"]}]},{"cell_type":"code","source":["#  Backbone model: ResNet-18 (nb params= 40,325,781)\n","backbone = resnet18(weights='IMAGENET1K_V1')\n","backbone = torch.nn.Sequential(*list(backbone.children())[:-2])  # Remove the final layers\n","backbone.out_channels = 512  # This is the final number of channels in ResNet-18 before the fully connected layers\n","\n","# Define the anchor generator with sizes suited for Faster R-CNN\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),) * 5)\n","\n","# Create the model\n","model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator)\n","model.to(device)\n","\n","# Optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","total_params = sum(p.numel() for p in model.parameters())\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","print(f\"Total number of parameters: {total_params}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEoI7QuxYAMK","executionInfo":{"status":"ok","timestamp":1730156747202,"user_tz":240,"elapsed":1137,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"dea6c65c-831d-4888-ccb1-b1f5ac4e902b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 126MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total number of parameters: 40325781\n"]}]},{"cell_type":"code","source":["# Backbone model: MobileNetV3 (nb params= 18,930,229)\n","model = fasterrcnn_mobilenet_v3_large_fpn(weights='FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT')\n","model.roi_heads.box_predictor = FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, num_classes=2)\n","model.to(device)\n","\n","# Optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","total_params = sum(p.numel() for p in model.parameters())\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","print(f\"Total number of parameters: {total_params}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMXZ3_bEYbAv","executionInfo":{"status":"ok","timestamp":1730165984404,"user_tz":240,"elapsed":382,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"885ad702-1fc4-4b7a-8a90-98ec3160d4c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 18930229\n"]}]},{"cell_type":"code","source":["## Training function\n","BATCH_SIZE = 12\n","data_loader = custom_data_loader(image_resized_folder, image_files, train_indices, labels_dict, BATCH_SIZE, TARGET_SIZE_IM)\n","\n","num_epochs = 1\n","epoch_losses = []\n","for epoch in range(num_epochs):\n","    model.train()\n","    batch_losses = []\n","    for batch_idx in tqdm(range(len(train_indices) // BATCH_SIZE)):\n","        images, targets = next(data_loader)  # Get the next batch\n","\n","        # Move images and targets to the appropriate device (CPU or GPU)\n","        images = images.to(device)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        # The model returns a dictionary of losses, sum them\n","        total_loss = sum(loss for loss in loss_dict.values())\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        optimizer.step()\n","        batch_losses.append(total_loss.item())\n","\n","    # Calculate the average loss for this epoch\n","    avg_epoch_loss = sum(batch_losses) / len(batch_losses)\n","    epoch_losses.append(avg_epoch_loss)  # Store the epoch loss\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"yaeBxZjvYkZr","executionInfo":{"status":"error","timestamp":1730167585632,"user_tz":240,"elapsed":1596756,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"7667dd73-b733-499f-9b1b-437d59aa5ab4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 24%|██▎       | 157/666 [26:36<1:26:15, 10.17s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-76a48e2883d7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the next batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move images and targets to the appropriate device (CPU or GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-7f257bc763c2>\u001b[0m in \u001b[0;36mcustom_data_loader\u001b[0;34m(folder_path, image_files, indices, labels_dict, batch_size, target_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                         \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Svae the model\n","\n","current_dir = curren_dir if USE_COLAB else os.getcwd()\n","# Fetch current date and time\n","now = datetime.now()\n","dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n","output_dir_name = \"output-\" + dt_string\n","\n","OUTPUT_DIR = os.path.join(current_dir, output_dir_name)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Save the model at the end of each epoch\n","ckpt_file_name = f\"{OUTPUT_DIR}/trained_FastRCNN_model.pth\"\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'loss': total_loss.item(),\n","}, ckpt_file_name)\n","print(f\"Final model saved to {ckpt_file_name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMoOMw8yIzBU","executionInfo":{"status":"ok","timestamp":1729286699309,"user_tz":240,"elapsed":7778,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"b5e26ef4-b276-40b5-b03a-d2074d4ad4f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final model saved to /content/drive/MyDrive/Colab Notebooks/output-18-10-2024-21-24-52/trained_FastRCNN_model.pth\n","Final model saved to /content/drive/MyDrive/Colab Notebooks/output-18-10-2024-21-24-56/trained_FastRCNN_model.pth\n"]}]},{"cell_type":"code","source":["def calculate_iou(box1, box2):\n","    \"\"\"\n","    Calculate Intersection over Union (IoU) between two bounding boxes.\n","    box1, box2: [xmin, ymin, xmax, ymax]\n","    Returns IoU value.\n","    \"\"\"\n","    # Determine the coordinates of the intersection rectangle\n","    x1_inter = max(box1[0], box2[0])\n","    y1_inter = max(box1[1], box2[1])\n","    x2_inter = min(box1[2], box2[2])\n","    y2_inter = min(box1[3], box2[3])\n","\n","    # Compute area of intersection\n","    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n","\n","    # Compute areas of both the bounding boxes\n","    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","\n","    # Compute the union area\n","    union_area = box1_area + box2_area - inter_area\n","\n","    # Compute the IoU\n","    iou = inter_area / union_area if union_area != 0 else 0\n","    return iou\n","\n","def evaluate_iou_for_batch(predictions, targets, iou_threshold=0.5):\n","    \"\"\"\n","    Evaluate IoU for a batch of predictions and targets.\n","    predictions: List of predicted bounding boxes\n","    targets: List of ground truth boxes\n","    iou_threshold: IoU threshold to consider a prediction correct\n","    Returns the IoU accuracy.\n","    \"\"\"\n","    correct_boxes = 0\n","    total_boxes = 0\n","\n","    for pred, target in zip(predictions, targets):\n","        pred_boxes = pred['boxes'].cpu().numpy()  # Predicted boxes\n","        true_boxes = target['boxes'].cpu().numpy()  # Ground truth boxes\n","\n","        for t_box in true_boxes:\n","            total_boxes += 1\n","            # Find the predicted box with the highest IoU for this true box\n","            iou_max = 0\n","            for p_box in pred_boxes:\n","                iou = calculate_iou(t_box, p_box)\n","                if iou > iou_max:\n","                    iou_max = iou\n","\n","            # Count as correct if IoU exceeds threshold\n","            if iou_max >= iou_threshold:\n","                correct_boxes += 1\n","\n","    iou_accuracy = correct_boxes / total_boxes if total_boxes > 0 else 0\n","    return iou_accuracy\n"],"metadata":{"id":"2Lhr_EOShzM-","executionInfo":{"status":"ok","timestamp":1730730213950,"user_tz":300,"elapsed":163,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["model.eval()  # Switch to evaluation mode\n","BATCH_SIZE = 12\n","# Create your custom data loader for validation (or training)\n","data_loader = custom_data_loader(zip_file_path, image_files, val_indices, df_labels, BATCH_SIZE, TARGET_SIZE_IM)\n","\n","with torch.no_grad():\n","    iou_accuracies = []\n","    for batch_idx in tqdm(range(len(val_indices) // BATCH_SIZE)):  # Loop through validation data\n","        images, targets = next(data_loader)  # Get the next batch\n","\n","        # Move to appropriate device\n","        images = images.to(device)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Forward pass to get predictions\n","        predictions = model(images)\n","\n","        # Calculate IoU for this batch\n","        iou_accuracy = evaluate_iou_for_batch(predictions, targets)\n","        iou_accuracies.append(iou_accuracy)\n","\n","    # Average IoU accuracy over all batches\n","    mean_iou_accuracy = sum(iou_accuracies) / len(iou_accuracies)\n","    print(f\"Mean IoU Accuracy: {mean_iou_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNlVVF2JjyMb","executionInfo":{"status":"ok","timestamp":1729287929518,"user_tz":240,"elapsed":497928,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"c0347c77-0371-43c1-cf5e-ac16bde43a4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 166/166 [08:17<00:00,  2.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Mean IoU Accuracy: 0.1739\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["crowdai = [\"Background\", \"Logo\"]\n","\n","# Set model to evaluation mode\n","model.eval()\n","\n","# Number of images to visualize\n","num_images = 1\n","\n","# Create your custom data loader (you might already have this part)\n","BATCH_SIZE = 12\n","data_loader = custom_data_loader(zip_file_path, image_files, train_indices, df_labels, BATCH_SIZE, TARGET_SIZE_IM)\n","\n","# Disable gradient calculations for evaluation\n","with torch.no_grad():\n","    for _ in range(num_images):  # Loop through the desired number of images\n","        # Get a batch of images and targets from the data loader\n","        images, targets = next(data_loader)\n","\n","        # Move images to the correct device\n","        images = images.to(device)\n","\n","        # Get model predictions (outputs are lists of dictionaries)\n","        predictions = model(images)\n","\n","        # Loop over each image in the batch\n","        for i in range(len(images)):\n","            # Move image back to CPU and convert to numpy for plotting\n","            img = images[i].cpu().permute(1, 2, 0).numpy()\n","\n","            # Extract predicted boxes, labels, and scores\n","            pred_boxes = predictions[i]['boxes'].cpu().numpy()  # Predicted bounding boxes\n","            pred_labels = predictions[i]['labels'].cpu().numpy()  # Predicted labels\n","            pred_scores = predictions[i]['scores'].cpu().numpy()  # Confidence scores for each box\n","\n","            max_index = pred_scores.argmax()  # Get the index of the highest score\n","            best_box = pred_boxes[max_index].reshape(1, -1)  # The best bounding box\n","            best_label = pred_labels[max_index].reshape(1)  # The best label\n","            best_score = pred_scores[max_index].reshape(1)  # The highest confidence score\n","\n","\n","            # Visualize the image with the best predicted box\n","            plot_image(img, best_box, best_score, best_label, crowdai)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"13kJwF5-8s7vofv6IrGzSVDl0QrjUGWOt"},"id":"09XiqxXmmyzG","executionInfo":{"status":"ok","timestamp":1729289081787,"user_tz":240,"elapsed":16101,"user":{"displayName":"Ethan Thomas Olivier","userId":"17536049062004988468"}},"outputId":"6014cbad-6285-4082-8dee-64c62d219f38"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"H5ublguVrNUc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"91745af26735e2fb8e91cfbd0927addffd49b8ef53a4792a6e721c1d45b56778"}}},"nbformat":4,"nbformat_minor":0}